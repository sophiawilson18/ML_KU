{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by opening the files and loading them into a Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01       0.00923671 0.00853168 0.00788046 0.00727895 0.00672336\n",
      " 0.00621017 0.00573615 0.00529832 0.0048939  0.00452035 0.00417532\n",
      " 0.00385662 0.00356225 0.00329034 0.0030392  0.00280722 0.00259294\n",
      " 0.00239503 0.00221222 0.00204336 0.00188739 0.00174333 0.00161026\n",
      " 0.00148735 0.00137382 0.00126896 0.0011721  0.00108264 0.001     ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N = np.linspace(0,30,30)\n",
    "lr = 1e-2 * 10 ** (-N / 30)\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "def load_data(name):\n",
    "    with h5py.File(f'{name}.h5', 'r') as f:\n",
    "        filename = name.split('/') [0]\n",
    "        return pandas.DataFrame(f[filename][:], dtype=np.float64)\n",
    "\n",
    "train = load_data('train')\n",
    "test  = load_data('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can verify the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Shape of training data set: {train.shape}')\n",
    "print (f'Shape of test data set: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the test set contains 2 columns less: `Truth` and `p_truth_E`.\n",
    "    \n",
    "Then we copy the variable list from the course website <https://www.nbi.dk/~petersen/Teaching/ML2023/InitialProject/VariableList.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['actualInteractionsPerCrossing', 'averageInteractionsPerCrossing', 'correctedActualMu', 'correctedAverageMu', 'correctedScaledActualMu', 'correctedScaledAverageMu', 'NvtxReco', 'p_nTracks', 'p_pt_track', 'p_eta', 'p_phi', 'p_charge', 'p_qOverP', 'p_z0', 'p_d0', 'p_sigmad0', 'p_d0Sig', 'p_EptRatio', 'p_dPOverP', 'p_z0theta', 'p_etaCluster', 'p_phiCluster', 'p_eCluster', 'p_rawEtaCluster', 'p_rawPhiCluster', 'p_rawECluster', 'p_eClusterLr0', 'p_eClusterLr1', 'p_eClusterLr2', 'p_eClusterLr3', 'p_etaClusterLr1', 'p_etaClusterLr2', 'p_phiClusterLr2', 'p_eAccCluster', 'p_f0Cluster', 'p_etaCalo', 'p_phiCalo', 'p_eTileGap3Cluster', 'p_cellIndexCluster', 'p_phiModCalo', 'p_etaModCalo', 'p_dPhiTH3', 'p_R12', 'p_fTG3', 'p_weta2', 'p_Reta', 'p_Rphi', 'p_Eratio', 'p_f1', 'p_f3', 'p_Rhad', 'p_Rhad1', 'p_deltaEta1', 'p_deltaPhiRescaled2', 'p_TRTPID', 'p_TRTTrackOccupancy', 'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_numberOfSCTHits', 'p_numberOfTRTHits', 'p_numberOfTRTXenonHits', 'p_chi2', 'p_ndof', 'p_SharedMuonTrack', 'p_E7x7_Lr2', 'p_E7x7_Lr3', 'p_E_Lr0_HiG', 'p_E_Lr0_LowG', 'p_E_Lr0_MedG', 'p_E_Lr1_HiG', 'p_E_Lr1_LowG', 'p_E_Lr1_MedG', 'p_E_Lr2_HiG', 'p_E_Lr2_LowG', 'p_E_Lr2_MedG', 'p_E_Lr3_HiG', 'p_E_Lr3_LowG', 'p_E_Lr3_MedG', 'p_ambiguityType', 'p_asy1', 'p_author', 'p_barys1', 'p_core57cellsEnergyCorrection', 'p_deltaEta0', 'p_deltaEta2', 'p_deltaEta3', 'p_deltaPhi0', 'p_deltaPhi1', 'p_deltaPhi2', 'p_deltaPhi3', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhiRescaled0', 'p_deltaPhiRescaled1', 'p_deltaPhiRescaled3', 'p_e1152', 'p_e132', 'p_e235', 'p_e255', 'p_e2ts1', 'p_ecore', 'p_emins1', 'p_etconeCorrBitset', 'p_ethad', 'p_ethad1', 'p_f1core', 'p_f3core', 'p_maxEcell_energy', 'p_maxEcell_gain', 'p_maxEcell_time', 'p_maxEcell_x', 'p_maxEcell_y', 'p_maxEcell_z', 'p_nCells_Lr0_HiG', 'p_nCells_Lr0_LowG', 'p_nCells_Lr0_MedG', 'p_nCells_Lr1_HiG', 'p_nCells_Lr1_LowG', 'p_nCells_Lr1_MedG', 'p_nCells_Lr2_HiG', 'p_nCells_Lr2_LowG', 'p_nCells_Lr2_MedG', 'p_nCells_Lr3_HiG', 'p_nCells_Lr3_LowG', 'p_nCells_Lr3_MedG', 'p_pos', 'p_pos7', 'p_poscs1', 'p_poscs2', 'p_ptconeCorrBitset', 'p_ptconecoreTrackPtrCorrection', 'p_r33over37allcalo', 'p_topoetconeCorrBitset', 'p_topoetconecoreConeEnergyCorrection', 'p_topoetconecoreConeSCEnergyCorrection', 'p_weta1', 'p_widths1', 'p_widths2', 'p_wtots1', 'p_e233', 'p_e237', 'p_e277', 'p_e2tsts1', 'p_ehad1', 'p_emaxs1', 'p_fracs1', 'p_DeltaE', 'p_E3x5_Lr0', 'p_E3x5_Lr1', 'p_E3x5_Lr2', 'p_E3x5_Lr3', 'p_E5x7_Lr0', 'p_E5x7_Lr1', 'p_E5x7_Lr2', 'p_E5x7_Lr3', 'p_E7x11_Lr0', 'p_E7x11_Lr1', 'p_E7x11_Lr2', 'p_E7x11_Lr3', 'p_E7x7_Lr0', 'p_E7x7_Lr1' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we divide the training data into data (`X`) and labels (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[all_features]\n",
    "y = train['Truth']\n",
    "X_test = test[all_features]\n",
    "\n",
    "print (f'Shape of X: {X.shape}')\n",
    "print (f'Shape of y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Random forest\n",
    "\n",
    "Identify (i.e. classify) electrons vs. non-electrons. This should be based on maximum 15 variables from the Electron Variable List. The target variable for this task is \"Truth\": 0 for non-electrons, 1 for electrons, but your identification should be continuous in [0,1]. We evaluate algorithm performance using Binary Cross Entropy loss (LogLoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tree Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "# Random seed\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into test data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: What features to use?\n",
    "\n",
    "Models to compare with:\n",
    "- A RFC with default settings and all features  -> accuracy at 0.94 \n",
    "- A RFC with default settings and top 15 features found from .feature_importances_ (MDI) -> accuracy at 0.934\n",
    "- A RFC with default settings and top 15 features found from SelectFromModel -> accuracy at 0.932\n",
    "\n",
    "Model:\n",
    "- A RFC with default settings and top 25 features found from SelectFromModel after which the top 10 correlated features are removed -> accuracy at 0.94018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_new = ['p_pt_track', 'p_d0', 'p_sigmad0', 'p_weta2', 'p_Reta', 'p_Eratio',\n",
    "       'p_Rhad', 'p_deltaEta1', 'p_deltaPhiRescaled2',\n",
    "       'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_E7x7_Lr3',\n",
    "       'p_ambiguityType', 'p_deltaEta2', 'p_e2tsts1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_split(X, y, features):\n",
    "    '''Splits data into positive and negative'''\n",
    "    df_X = pd.DataFrame(X[features]) \n",
    "    df_1 = df_X.loc[y==1]\n",
    "    df_0 = df_X.loc[y==0]\n",
    "    return df_X, df_1, df_0\n",
    "\n",
    "def corr_plot(df, ax, title):\n",
    "    '''Correlation plot'''\n",
    "    sns.heatmap(df.corr(), mask=np.zeros_like(df.corr(), dtype=bool), cbar_kws={\"shrink\": 0.65},\n",
    "            cmap=sns.diverging_palette(220, 10, as_cmap=True), vmin=-1.0, vmax=1.0, square=True, ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "def drop_high_corr(df, cut=0.95):\n",
    "    '''Drops features with Pearson correlation > cut'''\n",
    "    cor_matrix = df.corr(method='pearson').abs() \n",
    "    upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool)) \n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > cut)] \n",
    "    df_new = df.drop(df[to_drop], axis=1) \n",
    "    features_new = df_new.keys()\n",
    "    return df_new, features_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Neural Network using TensorFlow\n",
    "\n",
    "Rule of thumbs for contructing a Neural Network (examples using Tensorflow)\n",
    "\n",
    "https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the network hyperparameters\n",
    "n_inputs = 15  # number of inputs: 15 features\n",
    "n_hidden1 = 64 # number of neurons in first hidden layer: 10\n",
    "n_hidden2 = 64 # number of neurons in second hidden layer: 10\n",
    "n_hidden3 = 32 # number of neurons in first hidden layer: 10\n",
    "n_hidden4 = 32 # number of neurons in first hidden layer: 10\n",
    "n_outputs = 1  # number of outputs: 1 (can be either 0 or 1)\n",
    "learning_rate = 0.001  # learning rate - where do I use this ??  maybe optimizer?\n",
    "init = tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network structure\n",
    "model_tf = Sequential([\n",
    "    Dense(n_inputs, activation='relu',  name = 'input_layer'), # kernel_initializer=init,\n",
    "    Dense(n_hidden1, activation='relu', name = 'hidden_layer_1'),\n",
    "    #Dropout(rate=0.2, noise_shape=None, seed=42),\n",
    "    Dense(n_hidden2, activation='relu', name = 'hidden_layer_2'),\n",
    "    #Dropout(rate=0.2, noise_shape=None, seed=42),\n",
    "    Dense(n_hidden3, activation='relu', name = 'hidden_layer_3'),\n",
    "    Dense(n_outputs, activation='sigmoid', name = 'output_layer')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling model\n",
    "model_tf.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(), # binary loss function\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['p_pt_track', 'p_d0', 'p_sigmad0', 'p_weta2', 'p_Reta', 'p_Eratio',\n",
    "       'p_Rhad', 'p_deltaEta1', 'p_deltaPhiRescaled2',\n",
    "       'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_E7x7_Lr3',\n",
    "       'p_ambiguityType', 'p_deltaEta2', 'p_e2tsts1']\n",
    "\n",
    "# Jakobs features\n",
    "#features = ['p_pt_track', 'p_r33over37allcalo', 'p_dPOverP', 'p_Reta','p_ambiguityType', 'p_Rhad', 'p_d0', 'p_deltaPhiFromLastMeasurement','p_ptconecoreTrackPtrCorrection', 'p_numberOfPixelHits', 'p_EptRatio','p_deltaPhiRescaled2', 'p_sigmad0', 'p_deltaEta1','p_numberOfInnermostPixelHits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_features = pd.DataFrame(features)\n",
    "#df_features.to_csv('Classification_SophiaWilson_TensorFlowNeuralNetwork.csv_VariableList.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train[features])\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "X_train_scaled.columns = features\n",
    "y_train = pd.DataFrame(np.array(y_train))\n",
    "\n",
    "X_val_scaled = scaler.transform(X_val[features])\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "X_val_scaled.columns = features\n",
    "y_val = pd.DataFrame(np.array(y_val))\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test[features])\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled)\n",
    "X_test_scaled.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "n_epochs = 15\n",
    "batch_size = 256 \n",
    "\n",
    "print('--------- TRAINING ---------')\n",
    "history = model_tf.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size, \n",
    "                       validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(n_epochs, history_, ax):\n",
    "    ax[0].plot(np.arange(n_epochs), history_.history['accuracy'], 'ko-', label='train accuracy', alpha=0.5)\n",
    "    ax[0].plot(np.arange(n_epochs), history_.history['val_accuracy'], 'ro-', label='validation accuracy', alpha=0.5)\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(np.arange(n_epochs), history_.history['loss'], 'ko-', label='train loss', alpha=0.5)\n",
    "    ax[1].plot(np.arange(n_epochs), history_.history['val_loss'], 'ro-', label='validation loss', alpha=0.5)\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(11, 4))\n",
    "plot_accuracy_loss(n_epochs, history, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: 0.1732 - accuracy: 0.9363 - val_loss: 0.1882 - val_accuracy: 0.9329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation data\n",
    "y_pred_nn = model_tf.predict(X_val_scaled)\n",
    "\n",
    "# FPR and TPR for classification using RFC and NN\n",
    "#fpr_rfc, tpr_rfc, _ = roc_curve(y_val, y_pred_rfc)    \n",
    "fpr_nn, tpr_nn, _   = roc_curve(y_val, y_pred_nn)      \n",
    "  \n",
    "# We can now calculate the AUC scores of these ROC-curves:\n",
    "#auc_score_rfc = auc(fpr_rfc, tpr_rfc)                         \n",
    "auc_score_nn  = auc(fpr_nn, tpr_nn)   \n",
    "\n",
    "# Plot the results:\n",
    "fig = plt.figure(figsize = [5,5])\n",
    "plt.title('ROC Comparison')\n",
    "plt.xlabel('False Postive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "#plt.plot(fpr_rfc, tpr_rfc, label = f'RFC (AUC = {auc_score_rfc:6.4f})')\n",
    "plt.plot(fpr_nn, tpr_nn, label = f'NN (AUC = {auc_score_nn:6.4f}')\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on TEST data\n",
    "y_test_pred_nn = model_tf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test_pred_nn, density=True, histtype='step', label='Test data prediction')\n",
    "plt.hist(y_pred_nn, density=True, histtype='step', label='Validation data prediction')\n",
    "plt.hist(y_val, density=True, histtype='step', label='Validation data true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_test_pred_nn)\n",
    "df.to_csv('Classification_SophiaWilson_TensorFlowNeuralNetwork.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn2 = y_pred_nn.copy()\n",
    "y_pred_nn2[y_pred_nn2 > 0.5] = 1\n",
    "y_pred_nn2[y_pred_nn2 < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_nn2, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
